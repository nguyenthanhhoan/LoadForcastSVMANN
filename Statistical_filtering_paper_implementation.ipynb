{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical filtering paper implementation\n",
    "Created by Duy Anh Pham\n",
    "## No installation requires. All the blocks of code are implements on Google Colab (colab.research.google.com)\n",
    "The reasons of using Google Colab are (1) it is free of charge, (2) it does not require you to install any further libraries to your systems (which is really troublesome sometimes) and (3) it provides 12 consecutive hours of free GPU.\n",
    "\n",
    "### Instructions:\n",
    "1. Go to colab.research.google.com (sign in with your gmail account)\n",
    "2. Upload this notebook to your working repository by clicking File - Upload notebook\n",
    "3. Change runtime: in order to use free GPU on google colab, please go to Runtime - Change runtime type - set Hardware accelerator to GPU\n",
    "4. Go through the following sections step by step to achieve the results in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ae860f596cf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "import tensorflow.contrib.metrics as metrics\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "from google.colab import files\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Set a fixed random value so that every time the code is running it will yield the same results.\n",
    "from numpy.random import seed\n",
    "seed(1611)\n",
    "\n",
    "#Set a fixed random value so that every time the code is running it will yield the same results.\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(3116)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone the repository to Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/a11to1n3/statisticalFilterANN.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the repository folder\n",
    "%cd statisticalFilterANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the neccessary modules in the cloned repository\n",
    "import dataWrangling\n",
    "import dataPCA\n",
    "import dataHourSplit\n",
    "import dataFilter\n",
    "import confidenceLevelFitness\n",
    "import modelBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload raw data in the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this and select the DayMarked.csv file in your computer\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the csv file, we wrangle the data into 133 elements as stated in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangledData = dataWrangling.csvToArray('DayMarked.csv')\n",
    "\n",
    "# Check for the shape of the array after wrangling the data\n",
    "print(\"The shape of the array is: {}\".format(np.shape(wrangledData)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the wrangled data, we split into Monday, Sunday and the remaining days (rests) and scaled it by the log function and taking difference between pairs of 2 consecutive days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_scaled is the scaled data conducted by the log and differencing functions\n",
    "# x_orig is the original data without being scaled\n",
    "# prex_orig is the original data without being scaled of the day before analyzing day\n",
    "mon_scaled, sun_scaled, rests_scaled, mon_orig, sun_orig, rests_orig, presun_orig, premon_orig = dataPCA.plotPCAandSplit(wrangledData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For either Monday or Sunday, split the data into 4 timespans\n",
    "- Monday: 0h - 4h, 5h - 6h, 7h - 16h, 17h - 23h\n",
    "- Sunday: 0h - 4h, 5h - 6h, 7h - 15h, 16h - 23h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_to4, mon_5to6, mon_7to16, mon_17to23 = dataHourSplit.splitToDifferentTimeSpan(mon_scaled, 'Mon')\n",
    "sun_to4, sun_5to6, sun_7to15, sun_16to23 = dataHourSplit.splitToDifferentTimeSpan(sun_scaled, 'Sun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data for each given confidence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monday data filtered with confidence level of 90\n",
    "mon_to4_90 = dataFilter.filterWithConfidenceLevel(mon_to4, 90)\n",
    "mon_5to6_90 = dataFilter.filterWithConfidenceLevel(mon_5to6, 90)\n",
    "mon_7to16_90 = dataFilter.filterWithConfidenceLevel(mon_7to16, 90)\n",
    "mon_17to23_90 = dataFilter.filterWithConfidenceLevel(mon_17to23, 90)\n",
    "\n",
    "# Monday data filtered with confidence level of 91\n",
    "mon_to4_91 = dataFilter.filterWithConfidenceLevel(mon_to4, 91)\n",
    "mon_5to6_91 = dataFilter.filterWithConfidenceLevel(mon_5to6, 91)\n",
    "mon_7to16_91 = dataFilter.filterWithConfidenceLevel(mon_7to16, 91)\n",
    "mon_17to23_91 = dataFilter.filterWithConfidenceLevel(mon_17to23, 91)\n",
    "\n",
    "# Monday data filtered with confidence level of 92\n",
    "mon_to4_92 = dataFilter.filterWithConfidenceLevel(mon_to4, 92)\n",
    "mon_5to6_92 = dataFilter.filterWithConfidenceLevel(mon_5to6, 92)\n",
    "mon_7to16_92 = dataFilter.filterWithConfidenceLevel(mon_7to16, 92)\n",
    "mon_17to23_92 = dataFilter.filterWithConfidenceLevel(mon_17to23, 92)\n",
    "\n",
    "# Monday data filtered with confidence level of 93\n",
    "mon_to4_93 = dataFilter.filterWithConfidenceLevel(mon_to4, 93)\n",
    "mon_5to6_93 = dataFilter.filterWithConfidenceLevel(mon_5to6, 93)\n",
    "mon_7to16_93 = dataFilter.filterWithConfidenceLevel(mon_7to16, 93)\n",
    "mon_17to23_93 = dataFilter.filterWithConfidenceLevel(mon_17to23, 93)\n",
    "\n",
    "# Monday data filtered with confidence level of 94\n",
    "mon_to4_94 = dataFilter.filterWithConfidenceLevel(mon_to4, 94)\n",
    "mon_5to6_94 = dataFilter.filterWithConfidenceLevel(mon_5to6, 94)\n",
    "mon_7to16_94 = dataFilter.filterWithConfidenceLevel(mon_7to16, 94)\n",
    "mon_17to23_94 = dataFilter.filterWithConfidenceLevel(mon_17to23, 94)\n",
    "\n",
    "# Monday data filtered with confidence level of 95\n",
    "mon_to4_95 = dataFilter.filterWithConfidenceLevel(mon_to4, 95)\n",
    "mon_5to6_95 = dataFilter.filterWithConfidenceLevel(mon_5to6, 95)\n",
    "mon_7to16_95 = dataFilter.filterWithConfidenceLevel(mon_7to16, 95)\n",
    "mon_17to23_95 = dataFilter.filterWithConfidenceLevel(mon_17to23, 95)\n",
    "\n",
    "# Monday data filtered with confidence level of 96\n",
    "mon_to4_96 = dataFilter.filterWithConfidenceLevel(mon_to4, 96)\n",
    "mon_5to6_96 = dataFilter.filterWithConfidenceLevel(mon_5to6, 96)\n",
    "mon_7to16_96 = dataFilter.filterWithConfidenceLevel(mon_7to16, 96)\n",
    "mon_17to23_96 = dataFilter.filterWithConfidenceLevel(mon_17to23, 96)\n",
    "\n",
    "# Monday data filtered with confidence level of 97\n",
    "mon_to4_97 = dataFilter.filterWithConfidenceLevel(mon_to4, 97)\n",
    "mon_5to6_97 = dataFilter.filterWithConfidenceLevel(mon_5to6, 97)\n",
    "mon_7to16_97 = dataFilter.filterWithConfidenceLevel(mon_7to16, 97)\n",
    "mon_17to23_97 = dataFilter.filterWithConfidenceLevel(mon_17to23, 97)\n",
    "\n",
    "# Monday data filtered with confidence level of 98\n",
    "mon_to4_98 = dataFilter.filterWithConfidenceLevel(mon_to4, 98)\n",
    "mon_5to6_98 = dataFilter.filterWithConfidenceLevel(mon_5to6, 98)\n",
    "mon_7to16_98 = dataFilter.filterWithConfidenceLevel(mon_7to16, 98)\n",
    "mon_17to23_98 = dataFilter.filterWithConfidenceLevel(mon_17to23, 98)\n",
    "\n",
    "# Monday data filtered with confidence level of 99\n",
    "mon_to4_99 = dataFilter.filterWithConfidenceLevel(mon_to4, 99)\n",
    "mon_5to6_99 = dataFilter.filterWithConfidenceLevel(mon_5to6, 99)\n",
    "mon_7to16_99 = dataFilter.filterWithConfidenceLevel(mon_7to16, 99)\n",
    "mon_17to23_99 = dataFilter.filterWithConfidenceLevel(mon_17to23, 99)\n",
    "\n",
    "# Monday data filtered with confidence level of 99.73\n",
    "mon_to4_3sigma = dataFilter.filterWithConfidenceLevel(mon_to4, 99.73)\n",
    "mon_5to6_3sigma = dataFilter.filterWithConfidenceLevel(mon_5to6, 99.73)\n",
    "mon_7to16_3sigma = dataFilter.filterWithConfidenceLevel(mon_7to16, 99.73)\n",
    "mon_17to23_3sigma = dataFilter.filterWithConfidenceLevel(mon_17to23, 99.73)\n",
    "\n",
    "# Monday data filtered with confidence level of 99.99366\n",
    "mon_to4_4sigma = dataFilter.filterWithConfidenceLevel(mon_to4, 99.99366)\n",
    "mon_5to6_4sigma = dataFilter.filterWithConfidenceLevel(mon_5to6, 99.99366)\n",
    "mon_7to16_4sigma = dataFilter.filterWithConfidenceLevel(mon_7to16, 99.99366)\n",
    "mon_17to23_4sigma = dataFilter.filterWithConfidenceLevel(mon_17to23, 99.99366)\n",
    "\n",
    "# Monday data filtered with confidence level of 99.99932\n",
    "mon_to4_4sigma5 = dataFilter.filterWithConfidenceLevel(mon_to4, 99.99932)\n",
    "mon_5to6_4sigma5 = dataFilter.filterWithConfidenceLevel(mon_5to6, 99.99932)\n",
    "mon_7to16_4sigma5 = dataFilter.filterWithConfidenceLevel(mon_7to16, 99.99932)\n",
    "mon_17to23_4sigma5 = dataFilter.filterWithConfidenceLevel(mon_17to23, 99.99932)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sunday data filtered with confidence level of 90\n",
    "sun_to4_90 = dataFilter.filterWithConfidenceLevel(sun_to4, 90)\n",
    "sun_5to6_90 = dataFilter.filterWithConfidenceLevel(sun_5to6, 90)\n",
    "sun_7to15_90 = dataFilter.filterWithConfidenceLevel(sun_7to15, 90)\n",
    "sun_16to23_90 = dataFilter.filterWithConfidenceLevel(sun_16to23, 90)\n",
    "\n",
    "# Sunday data filtered with confidence level of 91\n",
    "sun_to4_91 = dataFilter.filterWithConfidenceLevel(sun_to4, 91)\n",
    "sun_5to6_91 = dataFilter.filterWithConfidenceLevel(sun_5to6, 91)\n",
    "sun_7to15_91 = dataFilter.filterWithConfidenceLevel(sun_7to15, 91)\n",
    "sun_16to23_91 = dataFilter.filterWithConfidenceLevel(sun_16to23, 91)\n",
    "\n",
    "# Sunday data filtered with confidence level of 92\n",
    "sun_to4_92 = dataFilter.filterWithConfidenceLevel(sun_to4, 92)\n",
    "sun_5to6_92 = dataFilter.filterWithConfidenceLevel(sun_5to6, 92)\n",
    "sun_7to15_92 = dataFilter.filterWithConfidenceLevel(sun_7to15, 92)\n",
    "sun_16to23_92 = dataFilter.filterWithConfidenceLevel(sun_16to23, 92)\n",
    "\n",
    "# Sunday data filtered with confidence level of 93\n",
    "sun_to4_93 = dataFilter.filterWithConfidenceLevel(sun_to4, 93)\n",
    "sun_5to6_93 = dataFilter.filterWithConfidenceLevel(sun_5to6, 93)\n",
    "sun_7to15_93 = dataFilter.filterWithConfidenceLevel(sun_7to15, 93)\n",
    "sun_16to23_93 = dataFilter.filterWithConfidenceLevel(sun_16to23, 93)\n",
    "\n",
    "# Sunday data filtered with confidence level of 94\n",
    "sun_to4_94 = dataFilter.filterWithConfidenceLevel(sun_to4, 94)\n",
    "sun_5to6_94 = dataFilter.filterWithConfidenceLevel(sun_5to6, 94)\n",
    "sun_7to15_94 = dataFilter.filterWithConfidenceLevel(sun_7to15, 94)\n",
    "sun_16to23_94 = dataFilter.filterWithConfidenceLevel(sun_16to23, 94)\n",
    "\n",
    "# Sunday data filtered with confidence level of 95\n",
    "sun_to4_95 = dataFilter.filterWithConfidenceLevel(sun_to4, 95)\n",
    "sun_5to6_95 = dataFilter.filterWithConfidenceLevel(sun_5to6, 95)\n",
    "sun_7to15_95 = dataFilter.filterWithConfidenceLevel(sun_7to15, 95)\n",
    "sun_16to23_95 = dataFilter.filterWithConfidenceLevel(sun_16to23, 95)\n",
    "\n",
    "# Sunday data filtered with confidence level of 96\n",
    "sun_to4_96 = dataFilter.filterWithConfidenceLevel(sun_to4, 96)\n",
    "sun_5to6_96 = dataFilter.filterWithConfidenceLevel(sun_5to6, 96)\n",
    "sun_7to15_96 = dataFilter.filterWithConfidenceLevel(sun_7to15, 96)\n",
    "sun_16to23_96 = dataFilter.filterWithConfidenceLevel(sun_16to23, 96)\n",
    "\n",
    "# Sunday data filtered with confidence level of 97\n",
    "sun_to4_97 = dataFilter.filterWithConfidenceLevel(sun_to4, 97)\n",
    "sun_5to6_97 = dataFilter.filterWithConfidenceLevel(sun_5to6, 97)\n",
    "sun_7to15_97 = dataFilter.filterWithConfidenceLevel(sun_7to15, 97)\n",
    "sun_16to23_97 = dataFilter.filterWithConfidenceLevel(sun_16to23, 97)\n",
    "\n",
    "# Sunday data filtered with confidence level of 98\n",
    "sun_to4_98 = dataFilter.filterWithConfidenceLevel(sun_to4, 98)\n",
    "sun_5to6_98 = dataFilter.filterWithConfidenceLevel(sun_5to6, 98)\n",
    "sun_7to15_98 = dataFilter.filterWithConfidenceLevel(sun_7to15, 98)\n",
    "sun_16to23_98 = dataFilter.filterWithConfidenceLevel(sun_16to23, 98)\n",
    "\n",
    "# Sunday data filtered with confidence level of 99\n",
    "sun_to4_99 = dataFilter.filterWithConfidenceLevel(sun_to4, 99)\n",
    "sun_5to6_99 = dataFilter.filterWithConfidenceLevel(sun_5to6, 99)\n",
    "sun_7to15_99 = dataFilter.filterWithConfidenceLevel(sun_7to15, 99)\n",
    "sun_16to23_99 = dataFilter.filterWithConfidenceLevel(sun_16to23, 99)\n",
    "\n",
    "# Sunday data filtered with confidence level of 99.73\n",
    "sun_to4_3sigma = dataFilter.filterWithConfidenceLevel(sun_to4, 99.73)\n",
    "sun_5to6_3sigma = dataFilter.filterWithConfidenceLevel(sun_5to6, 99.73)\n",
    "sun_7to15_3sigma = dataFilter.filterWithConfidenceLevel(sun_7to15, 99.73)\n",
    "sun_16to23_3sigma = dataFilter.filterWithConfidenceLevel(sun_16to23, 99.73)\n",
    "\n",
    "# Sunday data filtered with confidence level of 99.99366\n",
    "sun_to4_4sigma = dataFilter.filterWithConfidenceLevel(sun_to4, 99.99366)\n",
    "sun_5to6_4sigma = dataFilter.filterWithConfidenceLevel(sun_5to6, 99.99366)\n",
    "sun_7to15_4sigma = dataFilter.filterWithConfidenceLevel(sun_7to15, 99.99366)\n",
    "sun_16to23_4sigma = dataFilter.filterWithConfidenceLevel(sun_16to23, 99.99366)\n",
    "\n",
    "# Sunday data filtered with confidence level of 99.99932\n",
    "sun_to4_4sigma5 = dataFilter.filterWithConfidenceLevel(sun_to4, 99.99932)\n",
    "sun_5to6_4sigma5 = dataFilter.filterWithConfidenceLevel(sun_5to6, 99.99932)\n",
    "sun_7to15_4sigma5 = dataFilter.filterWithConfidenceLevel(sun_7to15, 99.99932)\n",
    "sun_16to23_4sigma5 = dataFilter.filterWithConfidenceLevel(sun_16to23, 99.99932)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining days data filtered with confidence level of 90\n",
    "rests_scaled_90 = dataFilter.filterWithConfidenceLevel(rests_scaled, 90)\n",
    "# Remaining days data filtered with confidence level of 91\n",
    "rests_scaled_91 = dataFilter.filterWithConfidenceLevel(rests_scaled, 91)\n",
    "# Remaining days data filtered with confidence level of 92\n",
    "rests_scaled_92 = dataFilter.filterWithConfidenceLevel(rests_scaled, 92)\n",
    "# Remaining days data filtered with confidence level of 93\n",
    "rests_scaled_93 = dataFilter.filterWithConfidenceLevel(rests_scaled, 93)\n",
    "# Remaining days data filtered with confidence level of 94\n",
    "rests_scaled_94 = dataFilter.filterWithConfidenceLevel(rests_scaled, 94)\n",
    "# Remaining days data filtered with confidence level of 95\n",
    "rests_scaled_95 = dataFilter.filterWithConfidenceLevel(rests_scaled, 95)\n",
    "# Remaining days data filtered with confidence level of 96\n",
    "rests_scaled_96 = dataFilter.filterWithConfidenceLevel(rests_scaled, 96)\n",
    "# Remaining days data filtered with confidence level of 97\n",
    "rests_scaled_97 = dataFilter.filterWithConfidenceLevel(rests_scaled, 97)\n",
    "# Remaining days data filtered with confidence level of 98\n",
    "rests_scaled_98 = dataFilter.filterWithConfidenceLevel(rests_scaled, 98)\n",
    "# Remaining days data filtered with confidence level of 99\n",
    "rests_scaled_99 = dataFilter.filterWithConfidenceLevel(rests_scaled, 99)\n",
    "# Remaining days data filtered with confidence level of 3sigma\n",
    "rests_scaled_3sigma = dataFilter.filterWithConfidenceLevel(rests_scaled, 99.73)\n",
    "# Remaining days data filtered with confidence level of 4sigma\n",
    "rests_scaled_4sigma = dataFilter.filterWithConfidenceLevel(rests_scaled, 99.99366)\n",
    "# Remaining days data filtered with confidence level of 4sigma5\n",
    "rests_scaled_4sigma5 = dataFilter.filterWithConfidenceLevel(rests_scaled, 99.99932)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the MAPE for each of the data intervals and find the confidence level that has the smallest MAPE in each data intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an empty MAPE object containing all the intervals with their confidence levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE = {\n",
    "    \"mon_to4\" : {\n",
    "        \"90\": 0,\n",
    "        \"91\": 0,\n",
    "        \"92\": 0,\n",
    "        \"93\": 0,\n",
    "        \"94\": 0,\n",
    "        \"95\": 0,\n",
    "        \"96\": 0,\n",
    "        \"97\": 0,\n",
    "        \"98\": 0,\n",
    "        \"99\": 0,\n",
    "        \"3sigma\": 0,\n",
    "        \"4sigma\": 0,\n",
    "        \"4sigma5\": 0\n",
    "    },\n",
    "    \"mon_5to6\" : {\n",
    "        \"90\": 0,\n",
    "        \"91\": 0,\n",
    "        \"92\": 0,\n",
    "        \"93\": 0,\n",
    "        \"94\": 0,\n",
    "        \"95\": 0,\n",
    "        \"96\": 0,\n",
    "        \"97\": 0,\n",
    "        \"98\": 0,\n",
    "        \"99\": 0,\n",
    "        \"3sigma\": 0,\n",
    "        \"4sigma\": 0,\n",
    "        \"4sigma5\": 0\n",
    "    },\n",
    "    \"mon_7to16\" : {\n",
    "        \"90\": 0,\n",
    "        \"91\": 0,\n",
    "        \"92\": 0,\n",
    "        \"93\": 0,\n",
    "        \"94\": 0,\n",
    "        \"95\": 0,\n",
    "        \"96\": 0,\n",
    "        \"97\": 0,\n",
    "        \"98\": 0,\n",
    "        \"99\": 0,\n",
    "        \"3sigma\": 0,\n",
    "        \"4sigma\": 0,\n",
    "        \"4sigma5\": 0\n",
    "    },\n",
    "    \"mon_16to23\" : {\n",
    "        \"90\": 0,\n",
    "        \"91\": 0,\n",
    "        \"92\": 0,\n",
    "        \"93\": 0,\n",
    "        \"94\": 0,\n",
    "        \"95\": 0,\n",
    "        \"96\": 0,\n",
    "        \"97\": 0,\n",
    "        \"98\": 0,\n",
    "        \"99\": 0,\n",
    "        \"3sigma\": 0,\n",
    "        \"4sigma\": 0,\n",
    "        \"4sigma5\": 0\n",
    "    },\n",
    "    \"sun_to4\" : {\n",
    "        \"90\": 0,\n",
    "        \"91\": 0,\n",
    "        \"92\": 0,\n",
    "        \"93\": 0,\n",
    "        \"94\": 0,\n",
    "        \"95\": 0,\n",
    "        \"96\": 0,\n",
    "        \"97\": 0,\n",
    "        \"98\": 0,\n",
    "        \"99\": 0,\n",
    "        \"3sigma\": 0,\n",
    "        \"4sigma\": 0,\n",
    "        \"4sigma5\": 0\n",
    "    },\n",
    "    \"sun_5to6\" : {\n",
    "        \"90\": 0,\n",
    "        \"91\": 0,\n",
    "        \"92\": 0,\n",
    "        \"93\": 0,\n",
    "        \"94\": 0,\n",
    "        \"95\": 0,\n",
    "        \"96\": 0,\n",
    "        \"97\": 0,\n",
    "        \"98\": 0,\n",
    "        \"99\": 0,\n",
    "        \"3sigma\": 0,\n",
    "        \"4sigma\": 0,\n",
    "        \"4sigma5\": 0\n",
    "    },\n",
    "    \"sun_7to15\" : {\n",
    "        \"90\": 0,\n",
    "        \"91\": 0,\n",
    "        \"92\": 0,\n",
    "        \"93\": 0,\n",
    "        \"94\": 0,\n",
    "        \"95\": 0,\n",
    "        \"96\": 0,\n",
    "        \"97\": 0,\n",
    "        \"98\": 0,\n",
    "        \"99\": 0,\n",
    "        \"3sigma\": 0,\n",
    "        \"4sigma\": 0,\n",
    "        \"4sigma5\": 0\n",
    "    },\n",
    "    \"sun_16to23\" : {\n",
    "        \"90\": 0,\n",
    "        \"91\": 0,\n",
    "        \"92\": 0,\n",
    "        \"93\": 0,\n",
    "        \"94\": 0,\n",
    "        \"95\": 0,\n",
    "        \"96\": 0,\n",
    "        \"97\": 0,\n",
    "        \"98\": 0,\n",
    "        \"99\": 0,\n",
    "        \"3sigma\": 0,\n",
    "        \"4sigma\": 0,\n",
    "        \"4sigma5\": 0\n",
    "    },\n",
    "    \"rests\": {\n",
    "        \"90\": 0,\n",
    "        \"91\": 0,\n",
    "        \"92\": 0,\n",
    "        \"93\": 0,\n",
    "        \"94\": 0,\n",
    "        \"95\": 0,\n",
    "        \"96\": 0,\n",
    "        \"97\": 0,\n",
    "        \"98\": 0,\n",
    "        \"99\": 0,\n",
    "        \"3sigma\": 0,\n",
    "        \"4sigma\": 0,\n",
    "        \"4sigma5\": 0\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE['mon_to4']['90'] = confidenceLevelFitness.calculateMAPE(mon_to4_90,mon_to4,100,2)\n",
    "MAPE['mon_to4']['91'] = confidenceLevelFitness.calculateMAPE(mon_to4_91,mon_to4,100,2)\n",
    "MAPE['mon_to4']['92'] = confidenceLevelFitness.calculateMAPE(mon_to4_92,mon_to4,100,2)\n",
    "MAPE['mon_to4']['93'] = confidenceLevelFitness.calculateMAPE(mon_to4_93,mon_to4,100,2)\n",
    "MAPE['mon_to4']['94'] = confidenceLevelFitness.calculateMAPE(mon_to4_94,mon_to4,100,2)\n",
    "MAPE['mon_to4']['95'] = confidenceLevelFitness.calculateMAPE(mon_to4_95,mon_to4,100,2)\n",
    "MAPE['mon_to4']['96'] = confidenceLevelFitness.calculateMAPE(mon_to4_96,mon_to4,100,2)\n",
    "MAPE['mon_to4']['97'] = confidenceLevelFitness.calculateMAPE(mon_to4_97,mon_to4,100,2)\n",
    "MAPE['mon_to4']['98'] = confidenceLevelFitness.calculateMAPE(mon_to4_98,mon_to4,100,2)\n",
    "MAPE['mon_to4']['99'] = confidenceLevelFitness.calculateMAPE(mon_to4_99,mon_to4,100,2)\n",
    "MAPE['mon_to4']['3sigma'] = confidenceLevelFitness.calculateMAPE(mon_to4_3sigma,mon_to4,100,2)\n",
    "MAPE['mon_to4']['4sigma'] = confidenceLevelFitness.calculateMAPE(mon_to4_4sigma,mon_to4,100,2)\n",
    "MAPE['mon_to4']['4sigma5'] = confidenceLevelFitness.calculateMAPE(mon_to4_4sigma5,mon_to4,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE['mon_5to6']['90'] = confidenceLevelFitness.calculateMAPE(mon_5to6_90,mon_5to6,100,2)\n",
    "MAPE['mon_5to6']['91'] = confidenceLevelFitness.calculateMAPE(mon_5to6_91,mon_5to6,100,2)\n",
    "MAPE['mon_5to6']['92'] = confidenceLevelFitness.calculateMAPE(mon_5to6_92,mon_5to6,100,2)\n",
    "MAPE['mon_5to6']['93'] = confidenceLevelFitness.calculateMAPE(mon_5to6_93,mon_5to6,100,2)\n",
    "MAPE['mon_5to6']['94'] = confidenceLevelFitness.calculateMAPE(mon_5to6_94,mon_5to6,100,2)\n",
    "MAPE['mon_5to6']['95'] = confidenceLevelFitness.calculateMAPE(mon_5to6_95,mon_5to6,100,2)\n",
    "MAPE['mon_5to6']['96'] = confidenceLevelFitness.calculateMAPE(mon_5to6_96,mon_5to6,100,2)\n",
    "MAPE['mon_5to6']['97'] = confidenceLevelFitness.calculateMAPE(mon_5to6_97,mon_5to6,100,2)\n",
    "MAPE['mon_5to6']['98'] = confidenceLevelFitness.calculateMAPE(mon_5to6_98,mon_5to6,100,2)\n",
    "MAPE['mon_5to6']['99'] = confidenceLevelFitness.calculateMAPE(mon_5to6_99,mon_5to6,100,2)\n",
    "MAPE['mon_5to6']['3sigma'] = confidenceLevelFitness.calculateMAPE(mon_5to6_3sigma,mon_5to6,100,2)\n",
    "MAPE['mon_5to6']['4sigma'] = confidenceLevelFitness.calculateMAPE(mon_5to6_4sigma,mon_5to6,100,2)\n",
    "MAPE['mon_5to6']['4sigma5'] = confidenceLevelFitness.calculateMAPE(mon_5to6_4sigma5,mon_5to6,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE['mon_7to16']['90'] = confidenceLevelFitness.calculateMAPE(mon_7to16_90,mon_7to16,100,2)\n",
    "MAPE['mon_7to16']['91'] = confidenceLevelFitness.calculateMAPE(mon_7to16_91,mon_7to16,100,2)\n",
    "MAPE['mon_7to16']['92'] = confidenceLevelFitness.calculateMAPE(mon_7to16_92,mon_7to16,100,2)\n",
    "MAPE['mon_7to16']['93'] = confidenceLevelFitness.calculateMAPE(mon_7to16_93,mon_7to16,100,2)\n",
    "MAPE['mon_7to16']['94'] = confidenceLevelFitness.calculateMAPE(mon_7to16_94,mon_7to16,100,2)\n",
    "MAPE['mon_7to16']['95'] = confidenceLevelFitness.calculateMAPE(mon_7to16_95,mon_7to16,100,2)\n",
    "MAPE['mon_7to16']['96'] = confidenceLevelFitness.calculateMAPE(mon_7to16_96,mon_7to16,100,2)\n",
    "MAPE['mon_7to16']['97'] = confidenceLevelFitness.calculateMAPE(mon_7to16_97,mon_7to16,100,2)\n",
    "MAPE['mon_7to16']['98'] = confidenceLevelFitness.calculateMAPE(mon_7to16_98,mon_7to16,100,2)\n",
    "MAPE['mon_7to16']['99'] = confidenceLevelFitness.calculateMAPE(mon_7to16_99,mon_7to16,100,2)\n",
    "MAPE['mon_7to16']['3sigma'] = confidenceLevelFitness.calculateMAPE(mon_7to16_3sigma,mon_7to16,100,2)\n",
    "MAPE['mon_7to16']['4sigma'] = confidenceLevelFitness.calculateMAPE(mon_7to16_4sigma,mon_7to16,100,2)\n",
    "MAPE['mon_7to16']['4sigma5'] = confidenceLevelFitness.calculateMAPE(mon_7to16_4sigma5,mon_7to16,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE['mon_17to23']['90'] = confidenceLevelFitness.calculateMAPE(mon_17to23_90,mon_17to23,100,2)\n",
    "MAPE['mon_17to23']['91'] = confidenceLevelFitness.calculateMAPE(mon_17to23_91,mon_17to23,100,2)\n",
    "MAPE['mon_17to23']['92'] = confidenceLevelFitness.calculateMAPE(mon_17to23_92,mon_17to23,100,2)\n",
    "MAPE['mon_17to23']['93'] = confidenceLevelFitness.calculateMAPE(mon_17to23_93,mon_17to23,100,2)\n",
    "MAPE['mon_17to23']['94'] = confidenceLevelFitness.calculateMAPE(mon_17to23_94,mon_17to23,100,2)\n",
    "MAPE['mon_17to23']['95'] = confidenceLevelFitness.calculateMAPE(mon_17to23_95,mon_17to23,100,2)\n",
    "MAPE['mon_17to23']['96'] = confidenceLevelFitness.calculateMAPE(mon_17to23_96,mon_17to23,100,2)\n",
    "MAPE['mon_17to23']['97'] = confidenceLevelFitness.calculateMAPE(mon_17to23_97,mon_17to23,100,2)\n",
    "MAPE['mon_17to23']['98'] = confidenceLevelFitness.calculateMAPE(mon_17to23_98,mon_17to23,100,2)\n",
    "MAPE['mon_17to23']['99'] = confidenceLevelFitness.calculateMAPE(mon_17to23_99,mon_17to23,100,2)\n",
    "MAPE['mon_17to23']['3sigma'] = confidenceLevelFitness.calculateMAPE(mon_17to23_3sigma,mon_17to23,100,2)\n",
    "MAPE['mon_17to23']['4sigma'] = confidenceLevelFitness.calculateMAPE(mon_17to23_4sigma,mon_17to23,100,2)\n",
    "MAPE['mon_17to23']['4sigma5'] = confidenceLevelFitness.calculateMAPE(mon_17to23_4sigma5,mon_17to23,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE['sun_to4']['90'] = confidenceLevelFitness.calculateMAPE(sun_to4_90,sun_to4,100,2)\n",
    "MAPE['sun_to4']['91'] = confidenceLevelFitness.calculateMAPE(sun_to4_91,sun_to4,100,2)\n",
    "MAPE['sun_to4']['92'] = confidenceLevelFitness.calculateMAPE(sun_to4_92,sun_to4,100,2)\n",
    "MAPE['sun_to4']['93'] = confidenceLevelFitness.calculateMAPE(sun_to4_93,sun_to4,100,2)\n",
    "MAPE['sun_to4']['94'] = confidenceLevelFitness.calculateMAPE(sun_to4_94,sun_to4,100,2)\n",
    "MAPE['sun_to4']['95'] = confidenceLevelFitness.calculateMAPE(sun_to4_95,sun_to4,100,2)\n",
    "MAPE['sun_to4']['96'] = confidenceLevelFitness.calculateMAPE(sun_to4_96,sun_to4,100,2)\n",
    "MAPE['sun_to4']['97'] = confidenceLevelFitness.calculateMAPE(sun_to4_97,sun_to4,100,2)\n",
    "MAPE['sun_to4']['98'] = confidenceLevelFitness.calculateMAPE(sun_to4_98,sun_to4,100,2)\n",
    "MAPE['sun_to4']['99'] = confidenceLevelFitness.calculateMAPE(sun_to4_99,sun_to4,100,2)\n",
    "MAPE['sun_to4']['3sigma'] = confidenceLevelFitness.calculateMAPE(sun_to4_3sigma,sun_to4,100,2)\n",
    "MAPE['sun_to4']['4sigma'] = confidenceLevelFitness.calculateMAPE(sun_to4_4sigma,sun_to4,100,2)\n",
    "MAPE['sun_to4']['4sigma5'] = confidenceLevelFitness.calculateMAPE(sun_to4_4sigma5,sun_to4,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE['sun_5to6']['90'] = confidenceLevelFitness.calculateMAPE(sun_5to6_90,sun_5to6,100,2)\n",
    "MAPE['sun_5to6']['91'] = confidenceLevelFitness.calculateMAPE(sun_5to6_91,sun_5to6,100,2)\n",
    "MAPE['sun_5to6']['92'] = confidenceLevelFitness.calculateMAPE(sun_5to6_92,sun_5to6,100,2)\n",
    "MAPE['sun_5to6']['93'] = confidenceLevelFitness.calculateMAPE(sun_5to6_93,sun_5to6,100,2)\n",
    "MAPE['sun_5to6']['94'] = confidenceLevelFitness.calculateMAPE(sun_5to6_94,sun_5to6,100,2)\n",
    "MAPE['sun_5to6']['95'] = confidenceLevelFitness.calculateMAPE(sun_5to6_95,sun_5to6,100,2)\n",
    "MAPE['sun_5to6']['96'] = confidenceLevelFitness.calculateMAPE(sun_5to6_96,sun_5to6,100,2)\n",
    "MAPE['sun_5to6']['97'] = confidenceLevelFitness.calculateMAPE(sun_5to6_97,sun_5to6,100,2)\n",
    "MAPE['sun_5to6']['98'] = confidenceLevelFitness.calculateMAPE(sun_5to6_98,sun_5to6,100,2)\n",
    "MAPE['sun_5to6']['99'] = confidenceLevelFitness.calculateMAPE(sun_5to6_99,sun_5to6,100,2)\n",
    "MAPE['sun_5to6']['3sigma'] = confidenceLevelFitness.calculateMAPE(sun_5to6_3sigma,sun_5to6,100,2)\n",
    "MAPE['sun_5to6']['4sigma'] = confidenceLevelFitness.calculateMAPE(sun_5to6_4sigma,sun_5to6,100,2)\n",
    "MAPE['sun_5to6']['4sigma5'] = confidenceLevelFitness.calculateMAPE(sun_5to6_4sigma5,sun_5to6,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE['sun_7to15']['90'] = confidenceLevelFitness.calculateMAPE(sun_7to15_90,sun_7to15,100,2)\n",
    "MAPE['sun_7to15']['91'] = confidenceLevelFitness.calculateMAPE(sun_7to15_91,sun_7to15,100,2)\n",
    "MAPE['sun_7to15']['92'] = confidenceLevelFitness.calculateMAPE(sun_7to15_92,sun_7to15,100,2)\n",
    "MAPE['sun_7to15']['93'] = confidenceLevelFitness.calculateMAPE(sun_7to15_93,sun_7to15,100,2)\n",
    "MAPE['sun_7to15']['94'] = confidenceLevelFitness.calculateMAPE(sun_7to15_94,sun_7to15,100,2)\n",
    "MAPE['sun_7to15']['95'] = confidenceLevelFitness.calculateMAPE(sun_7to15_95,sun_7to15,100,2)\n",
    "MAPE['sun_7to15']['96'] = confidenceLevelFitness.calculateMAPE(sun_7to15_96,sun_7to15,100,2)\n",
    "MAPE['sun_7to15']['97'] = confidenceLevelFitness.calculateMAPE(sun_7to15_97,sun_7to15,100,2)\n",
    "MAPE['sun_7to15']['98'] = confidenceLevelFitness.calculateMAPE(sun_7to15_98,sun_7to15,100,2)\n",
    "MAPE['sun_7to15']['99'] = confidenceLevelFitness.calculateMAPE(sun_7to15_99,sun_7to15,100,2)\n",
    "MAPE['sun_7to15']['3sigma'] = confidenceLevelFitness.calculateMAPE(sun_7to15_3sigma,sun_7to15,100,2)\n",
    "MAPE['sun_7to15']['4sigma'] = confidenceLevelFitness.calculateMAPE(sun_7to15_4sigma,sun_7to15,100,2)\n",
    "MAPE['sun_7to15']['4sigma5'] = confidenceLevelFitness.calculateMAPE(sun_7to15_4sigma5,sun_7to15,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE['sun_16to23']['90'] = confidenceLevelFitness.calculateMAPE(sun_16to23_90,sun_16to23,100,2)\n",
    "MAPE['sun_16to23']['91'] = confidenceLevelFitness.calculateMAPE(sun_16to23_91,sun_16to23,100,2)\n",
    "MAPE['sun_16to23']['92'] = confidenceLevelFitness.calculateMAPE(sun_16to23_92,sun_16to23,100,2)\n",
    "MAPE['sun_16to23']['93'] = confidenceLevelFitness.calculateMAPE(sun_16to23_93,sun_16to23,100,2)\n",
    "MAPE['sun_16to23']['94'] = confidenceLevelFitness.calculateMAPE(sun_16to23_94,sun_16to23,100,2)\n",
    "MAPE['sun_16to23']['95'] = confidenceLevelFitness.calculateMAPE(sun_16to23_95,sun_16to23,100,2)\n",
    "MAPE['sun_16to23']['96'] = confidenceLevelFitness.calculateMAPE(sun_16to23_96,sun_16to23,100,2)\n",
    "MAPE['sun_16to23']['97'] = confidenceLevelFitness.calculateMAPE(sun_16to23_97,sun_16to23,100,2)\n",
    "MAPE['sun_16to23']['98'] = confidenceLevelFitness.calculateMAPE(sun_16to23_98,sun_16to23,100,2)\n",
    "MAPE['sun_16to23']['99'] = confidenceLevelFitness.calculateMAPE(sun_16to23_99,sun_16to23,100,2)\n",
    "MAPE['sun_16to23']['3sigma'] = confidenceLevelFitness.calculateMAPE(sun_16to23_3sigma,sun_16to23,100,2)\n",
    "MAPE['sun_16to23']['4sigma'] = confidenceLevelFitness.calculateMAPE(sun_16to23_4sigma,sun_16to23,100,2)\n",
    "MAPE['sun_16to23']['4sigma5'] = confidenceLevelFitness.calculateMAPE(sun_16to23_4sigma5,sun_16to23,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE['rests']['90'] = confidenceLevelFitness.calculateMAPE(rests_scaled_90,rests_scaled,100,4)\n",
    "MAPE['rests']['91'] = confidenceLevelFitness.calculateMAPE(rests_scaled_91,rests_scaled,100,4)\n",
    "MAPE['rests']['92'] = confidenceLevelFitness.calculateMAPE(rests_scaled_92,rests_scaled,100,4)\n",
    "MAPE['rests']['93'] = confidenceLevelFitness.calculateMAPE(rests_scaled_93,rests_scaled,100,4)\n",
    "MAPE['rests']['94'] = confidenceLevelFitness.calculateMAPE(rests_scaled_94,rests_scaled,100,4)\n",
    "MAPE['rests']['95'] = confidenceLevelFitness.calculateMAPE(rests_scaled_95,rests_scaled,100,4)\n",
    "MAPE['rests']['96'] = confidenceLevelFitness.calculateMAPE(rests_scaled_96,rests_scaled,100,4)\n",
    "MAPE['rests']['97'] = confidenceLevelFitness.calculateMAPE(rests_scaled_97,rests_scaled,100,4)\n",
    "MAPE['rests']['98'] = confidenceLevelFitness.calculateMAPE(rests_scaled_98,rests_scaled,100,4)\n",
    "MAPE['rests']['99'] = confidenceLevelFitness.calculateMAPE(rests_scaled_99,rests_scaled,100,4)\n",
    "MAPE['rests']['3sigma'] = confidenceLevelFitness.calculateMAPE(rests_scaled_3sigma,rests_scaled,100,4)\n",
    "MAPE['rests']['4sigma'] = confidenceLevelFitness.calculateMAPE(rests_scaled_4sigma,rests_scaled,100,4)\n",
    "MAPE['rests']['4sigma5'] = confidenceLevelFitness.calculateMAPE(rests_scaled_4sigma5,rests_scaled,100,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the confidence level that has the smallest MAPE in each data intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method: Find the minimum MAPE in each data intervals at every confidence level  and return the overall average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSmallestMAPE(dataInterval):\n",
    "    if list(MAPE[dataInterval].values()).index(min(list(MAPE[dataInterval].values()))) + 90 < 100:\n",
    "        #print(list(MAPE[dataInterval].values()).index(min(list(MAPE[dataInterval].values()))) + 90)\n",
    "        return list(MAPE[dataInterval].values()).index(min(list(MAPE[dataInterval].values()))) + 90\n",
    "    elif list(MAPE[dataInterval].values()).index(min(list(MAPE[dataInterval].values()))) + 90 == 100:\n",
    "        #print('3sigma')\n",
    "        return '3sigma'\n",
    "    elif list(MAPE[dataInterval].values()).index(min(list(MAPE[dataInterval].values()))) + 90 == 101:\n",
    "        #print('4sigma')\n",
    "        return '4sigma'\n",
    "    elif list(MAPE[dataInterval].values()).index(min(list(MAPE[dataInterval].values()))) + 90 == 102:\n",
    "        #print('4sigma5')\n",
    "        return '4sigma5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum = []\n",
    "minimum.append(findSmallestMAPE('mon_to4'))\n",
    "minimum.append(findSmallestMAPE('mon_5to6'))\n",
    "minimum.append(findSmallestMAPE('mon_7to16'))\n",
    "minimum.append(findSmallestMAPE('mon_17to23'))\n",
    "minimum.append(findSmallestMAPE('sun_to4'))\n",
    "minimum.append(findSmallestMAPE('sun_5to6'))\n",
    "minimum.append(findSmallestMAPE('sun_7to15'))\n",
    "minimum.append(findSmallestMAPE('sun_16to23'))\n",
    "minimum.append(findSmallestMAPE('rests'))\n",
    "\n",
    "print(\"The overall average best confidence level is {}\".format(np.floor(np.mean(minimum))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After determining the best confidence level, use the filtered data with that confidence level (here it is 95%) to forecast and compare with the real future values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build ANN model, train and test for Monday data with 95% level of confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather all the intervals within the day and integrate it to an assembled array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mon = np.zeros((mon_to4_95.shape[0],24,133))\n",
    "data_mon[:,:5,:] = mon_to4_95\n",
    "data_mon[:,5:7,:] = mon_5to6_95\n",
    "data_mon[:,7:17,:] = mon_7to16_95\n",
    "data_mon[:,17:,:] = mon_17to23_95\n",
    "data_mon_test = np.zeros((mon_to4.shape[0],24,133))\n",
    "data_mon_test[:,:5,:] = mon_to4\n",
    "data_mon_test[:,5:7,:] = mon_5to6\n",
    "data_mon_test[:,7:17,:] = mon_7to16\n",
    "data_mon_test[:,17:,:] = mon_17to23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = data_mon[:-int(0.1*len(data_mon))-1]\n",
    "out = data_mon[1:-int(0.1*len(data_mon))]\n",
    "test_inp = data_mon_test[-int(0.1*len(data_mon)):-1]\n",
    "test_out = data_mon_test[-int(0.1*len(data_mon))+1:]\n",
    "pretest_inp = premon_orig[-int(0.1*len(data_mon))+1:]\n",
    "pretest_out = mon_orig[-int(0.1*len(data_mon))+1:]\n",
    "x_train, x_val, y_train, y_val = train_test_split(inp,out,\n",
    "                                                  random_state=42, test_size =0.33,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelBuilder.build(data_mon.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mae', optimizer='adam', metrics=['mse','mape'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(x_train, y_train[:,:,0], epochs=150, batch_size=2, \n",
    "                    validation_data=(x_val, y_val[:,:,0]), verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG = []\n",
    "pred = []\n",
    "plot_val = []\n",
    "for i in range(len(test_inp)):\n",
    "    test_pred = model.predict(test_inp[i].reshape(-1,data_mon.shape[1],133))\n",
    "    pred.append(test_pred)\n",
    "    real_pred = np.log(pretest_inp[i,:,0].T) + test_pred\n",
    "    real_pred = np.e**real_pred\n",
    "    #print(abs((real_pred.T.reshape(-1)-pretest_out[i,:,0].reshape(-1)))/pretest_out[i,:,0].reshape(-1))\n",
    "    plot_val.append([real_pred.T.reshape(-1),pretest_out[i,:,0].reshape(-1)])\n",
    "    AVG.append(np.sum(abs((real_pred.T.reshape(-1)-pretest_out[i,:,0].reshape(-1)))/pretest_out[i,:,0].reshape(-1))*100/24)\n",
    "AVG = np.array(AVG)\n",
    "pred=np.array(pred)\n",
    "print(np.sum(AVG)/len(pretest_inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast with ARIMA for Monday data with 95% level of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jul 15 14:32:54 2019\n",
    "\n",
    "@author: duypham\n",
    "\"\"\"\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "AVG=[]\n",
    "history = [x for x in data_mon[:,:,0].reshape(-1)]\n",
    "predictions = list()\n",
    "plot_val1=[]\n",
    "for t in range(len(pretest_out[:,:,0].reshape(-1))):\n",
    "    model = ARIMA(history, order=(6,0,0))\n",
    "    model_fit = model.fit(disp=1)\n",
    "    output = model_fit.forecast()\n",
    "  \n",
    "  \n",
    "    yhat = output[0]\n",
    "    test_pred = yhat\n",
    "    real_pred = np.log(pretest_inp[:,:,0].reshape(-1)[t])+test_pred\n",
    "    real_pred = np.e**real_pred\n",
    "    print(test_pred.shape)\n",
    "    #pred.append(test_pred)\n",
    "    print(abs((real_pred-pretest_out[:,:,0].reshape(-1)[t]))/pretest_out[:,:,0].reshape(-1)[t])\n",
    "    plot_val1.append([real_pred,pretest_out[:,:,0].reshape(-1)[t]])\n",
    "    AVG.append(abs((real_pred-pretest_out[:,:,0].reshape(-1)[t]))/pretest_out[:,:,0].reshape(-1)[t]*100)\n",
    "AVG = np.array(AVG)\n",
    "#pred=np.array(pred)\n",
    "print(np.mean(AVG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph the plots of ANN and ARIMA for Monday data with 95% confidence level for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(15,10))\n",
    "plot_val = np.array(plot_val)\n",
    "plot_val1 = np.array(plot_val1)\n",
    "truth = plot_val[:,1,:].reshape(-1)\n",
    "forecast1 = plot_val[:,0,:].reshape(-1)\n",
    "forecast2 = plot_val1[:,0].reshape(-1)\n",
    "plt.plot(truth[:7*24],'b',label='Actual')\n",
    "plt.plot(forecast1[:7*24],'r-',label='Forecast from ANN')\n",
    "plt.plot(forecast2[:7*24],'v-',label='Forecast from ARIMA')\n",
    "plt.legend()\n",
    "plt.title(\"Actual and Predict values of the data Monday using proposed method on ANN and ARIMA with 95% confidence level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build ANN model, train and test for Sunday data with 95% level of confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather all the intervals within the day and integrate it to an assembled array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sun = np.zeros((sun_to4_95.shape[0],24,133))\n",
    "data_sun[:,:5,:] = sun_to4_95\n",
    "data_sun[:,5:7,:] = sun_5to6_95\n",
    "data_sun[:,7:16,:] = sun_7to15_95\n",
    "data_sun[:,16:,:] = sun_16to23_95\n",
    "data_sun_test = np.zeros((sun_to4.shape[0],24,133))\n",
    "data_sun_test[:,:5,:] = sun_to4\n",
    "data_sun_test[:,5:7,:] = sun_5to6\n",
    "data_sun_test[:,7:16,:] = sun_7to15\n",
    "data_sun_test[:,16:,:] = sun_16to23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = data_sun[:-int(0.1*len(data_sun))-1]\n",
    "out = data_sun[1:-int(0.1*len(data_sun))]\n",
    "test_inp = data_sun_test[-int(0.1*len(data_sun)):-1]\n",
    "test_out = data_sun_test[-int(0.1*len(data_sun))+1:]\n",
    "pretest_inp = presun_orig[-int(0.1*len(data_sun))+1:]\n",
    "pretest_out = sun_orig[-int(0.1*len(data_sun))+1:]\n",
    "x_train, x_val, y_train, y_val = train_test_split(inp,out,\n",
    "                                                  random_state=42, test_size =0.33,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelBuilder.build(data_sun.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mae', optimizer='adam', metrics=['mse','mape'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(x_train, y_train[:,:,0], epochs=150, batch_size=2, \n",
    "                    validation_data=(x_val, y_val[:,:,0]), verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG = []\n",
    "pred = []\n",
    "plot_val = []\n",
    "for i in range(len(test_inp)):\n",
    "    test_pred = model.predict(test_inp[i].reshape(-1,data_sun.shape[1],133))\n",
    "    pred.append(test_pred)\n",
    "    real_pred = np.log(pretest_inp[i,:,0].T) + test_pred\n",
    "    real_pred = np.e**real_pred\n",
    "    #print(abs((real_pred.T.reshape(-1)-pretest_out[i,:,0].reshape(-1)))/pretest_out[i,:,0].reshape(-1))\n",
    "    plot_val.append([real_pred.T.reshape(-1),pretest_out[i,:,0].reshape(-1)])\n",
    "    AVG.append(np.sum(abs((real_pred.T.reshape(-1)-pretest_out[i,:,0].reshape(-1)))/pretest_out[i,:,0].reshape(-1))*100/24)\n",
    "AVG = np.array(AVG)\n",
    "pred=np.array(pred)\n",
    "print(np.sum(AVG)/len(pretest_inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast with ARIMA for Sunday data with 95% level of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jul 15 14:32:54 2019\n",
    "\n",
    "@author: duypham\n",
    "\"\"\"\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "AVG=[]\n",
    "history = [x for x in data_sun[:,:,0].reshape(-1)]\n",
    "predictions = list()\n",
    "plot_val1=[]\n",
    "for t in range(len(pretest_out[:,:,0].reshape(-1))):\n",
    "    model = ARIMA(history, order=(6,0,0))\n",
    "    model_fit = model.fit(disp=1)\n",
    "    output = model_fit.forecast()\n",
    "  \n",
    "  \n",
    "    yhat = output[0]\n",
    "    test_pred = yhat\n",
    "    real_pred = np.log(pretest_inp[:,:,0].reshape(-1)[t])+test_pred\n",
    "    real_pred = np.e**real_pred\n",
    "    print(test_pred.shape)\n",
    "    #pred.append(test_pred)\n",
    "    print(abs((real_pred-pretest_out[:,:,0].reshape(-1)[t]))/pretest_out[:,:,0].reshape(-1)[t])\n",
    "    plot_val1.append([real_pred,pretest_out[:,:,0].reshape(-1)[t]])\n",
    "    AVG.append(abs((real_pred-pretest_out[:,:,0].reshape(-1)[t]))/pretest_out[:,:,0].reshape(-1)[t]*100)\n",
    "AVG = np.array(AVG)\n",
    "#pred=np.array(pred)\n",
    "print(np.mean(AVG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph the plots of ANN and ARIMA for Sunday data with 95% confidence level for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(15,10))\n",
    "plot_val = np.array(plot_val)\n",
    "plot_val1 = np.array(plot_val1)\n",
    "truth = plot_val[:,1,:].reshape(-1)\n",
    "forecast1 = plot_val[:,0,:].reshape(-1)\n",
    "forecast2 = plot_val1[:,0].reshape(-1)\n",
    "plt.plot(truth[:7*24],'b',label='Actual')\n",
    "plt.plot(forecast1[:7*24],'r-',label='Forecast from ANN')\n",
    "plt.plot(forecast2[:7*24],'v-',label='Forecast from ARIMA')\n",
    "plt.legend()\n",
    "plt.title(\"True and Predict values of the data Sunday using proposed method on ANN and ARIMA with 95% confidence level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build ANN model, train and test for Tuesday to Saturday data with 95% level of confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the confidence level and reassign the variable for 'rests'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rests = rests_scaled_95\n",
    "data_rests_test = rests_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = data_rests[:-int(0.1*len(data_rests))-1]\n",
    "out = data_rests[1:-int(0.1*len(data_rests))]\n",
    "test_inp = data_rests_test[-int(0.1*len(data_rests)):-1]\n",
    "test_out = data_rests_test[-int(0.1*len(data_rests))+1:]\n",
    "pretest_inp = rests_orig[:-1][-int(0.1*len(data_rests))+1:]\n",
    "pretest_out = rests_orig[1:][-int(0.1*len(data_rests))+1:]\n",
    "x_train, x_val, y_train, y_val = train_test_split(inp,out,\n",
    "                                                  random_state=42, test_size =0.33,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelBuilder.build(data_rests.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mae', optimizer='adam', metrics=['mse','mape'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(x_train, y_train[:,:,0], epochs=150, batch_size=4, \n",
    "                    validation_data=(x_val, y_val[:,:,0]), verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG = []\n",
    "pred = []\n",
    "plot_val = []\n",
    "for i in range(len(test_inp)):\n",
    "    test_pred = model.predict(test_inp[i].reshape(-1,data_rests.shape[1],133))\n",
    "    pred.append(test_pred)\n",
    "    real_pred = np.log(pretest_inp[i,:,0].T) + test_pred\n",
    "    real_pred = np.e**real_pred\n",
    "    #print(abs((real_pred.T.reshape(-1)-pretest_out[i,:,0].reshape(-1)))/pretest_out[i,:,0].reshape(-1))\n",
    "    plot_val.append([real_pred.T.reshape(-1),pretest_out[i,:,0].reshape(-1)])\n",
    "    AVG.append(np.sum(abs((real_pred.T.reshape(-1)-pretest_out[i,:,0].reshape(-1)))/pretest_out[i,:,0].reshape(-1))*100/24)\n",
    "AVG = np.array(AVG)\n",
    "pred=np.array(pred)\n",
    "print(np.sum(AVG)/len(pretest_inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast with ARIMA for 'rests' data with 95% level of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jul 15 14:32:54 2019\n",
    "\n",
    "@author: duypham\n",
    "\"\"\"\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "AVG=[]\n",
    "history = [x for x in data_rests[:,:,0].reshape(-1)]\n",
    "predictions = list()\n",
    "plot_val1=[]\n",
    "for t in range(len(pretest_out[:,:,0].reshape(-1))):\n",
    "    model = ARIMA(history, order=(6,0,0))\n",
    "    model_fit = model.fit(disp=1)\n",
    "    output = model_fit.forecast()\n",
    "  \n",
    "  \n",
    "    yhat = output[0]\n",
    "    test_pred = yhat\n",
    "    real_pred = np.log(pretest_inp[:,:,0].reshape(-1)[t])+test_pred\n",
    "    real_pred = np.e**real_pred\n",
    "    print(test_pred.shape)\n",
    "    #pred.append(test_pred)\n",
    "    print(abs((real_pred-pretest_out[:,:,0].reshape(-1)[t]))/pretest_out[:,:,0].reshape(-1)[t])\n",
    "    plot_val1.append([real_pred,pretest_out[:,:,0].reshape(-1)[t]])\n",
    "    AVG.append(abs((real_pred-pretest_out[:,:,0].reshape(-1)[t]))/pretest_out[:,:,0].reshape(-1)[t]*100)\n",
    "AVG = np.array(AVG)\n",
    "#pred=np.array(pred)\n",
    "print(np.mean(AVG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph the plots of ANN and ARIMA for 'rests' data with 95% confidence level for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(15,10))\n",
    "truth = plot_val[:,1,:].reshape(-1)\n",
    "plot_val = np.array(plot_val)\n",
    "plot_val1 = np.array(plot_val1)\n",
    "forecast1 = plot_val[:,0,:].reshape(-1)\n",
    "forecast2 = plot_val1[:,0].reshape(-1)\n",
    "plt.plot(truth[:7*24],'b',label='Actual')\n",
    "plt.plot(forecast1[:7*24],'r-',label='Forecast from ANN')\n",
    "plt.plot(forecast2[:7*24],'v-',label='Forecast from ARIMA')\n",
    "plt.legend()\n",
    "plt.title(\"True and Predict values of the data Tuesday to Saturday using proposed method on ANN and ARIMA with 95% confidence level\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
